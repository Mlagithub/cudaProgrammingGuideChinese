\chapter{性能指南}
\section{总体性能优化策略}

性能优化始终围绕三个基本策略：
\begin{itemize}
\item 最大化并行执行以获得最大利用率；
\item 优化存储器使用以获得最大存储器吞吐量；
\item 优化指令使用获得最大指令吞吐量。
\end{itemize}
对于应用的某个特定部分，什么策略会产生最好的性能收益依赖于这些部分的性能限制；如，优化存储器访问限制的内核的指令使用不会产生明显的效果。优化努力应当不变地被测试到和监视到的性能限制所定向，如使用CUDA profiler。另外比较某个特定内核浮点操作吞吐量或存储器吞吐量（更有意义）与对应的理论峰值吞吐量的差异可指出性能提升空间。

\section{最大化利用率}

尽量展现并行性并且有效的将并行性映射到系统的各个组件上以保证它们大部分时间都在工作，为了最大利用率应用应当以这为准则构建。

\subsection{应用层次}

在高层次上，应用应当通过使用异步函数调用和\ref{sec:stream}描述的流来最大化主机，设备和连接主机和设备的总线的并行执行。应当把每个处理器最擅长的任务分配给它：串行工作分配给主机；并行工作分配给设备。

对于并行工作，在算法中，由于某些线程为了与其它线程共享数据而同步导致并行性中断的点，有两种情况：或那些线程属于同一个块，这种情况下，它们只要使用{\_}{\_}syncthreads()和在同一个内核调用中使用共享存储器共享数据，或属于不同的块，这种情况下，它们必须使用两个不同的内核调用以通过全局存储器共享数据，一个内核将数据写入全局存储器，另一个从全局存储器中读。第二种情况优化得比较差，因为它增加了额外的内核调用消耗和全局存储器通信量。为了减弱这种影响，应当以一种将线程间通信局限在一个块的方式将算法映射到CUDA编程模型。

\subsection{设备层次}

在低层次，应用应当最大化设备内多处理器间的并行执行。

对于计算能力1.x的设备，一次只允许一个内核在设备上执行，所以内核要发射的块数至少要和多处理器数一样多。

对于计算能力2.x的设备，多个内核可在设备上并发执行，因此可以使用流来发射多个内核并发执行（参见\ref{sec:concurrentKernels}）来获得最大利用率。（译者注：此时要注意这些内核之间要没有依赖）

\subsection{多处理器层次}
\label{sec:multiprocessor}

在一个更低的层次，应用应当最大化多处理器内部的各种功能单元的并行执行。

如\ref{sec:hardwarethread}节所描述的，GPU多处理器依赖线程级并行来最大化其功能单元的利用。利用率和常驻束数量直接相关。在每次指令发射时，束调度器选择一个已准备好执行的束并将下个指令发射给束内的活动线程。束准备执行下一条指令花费的时钟周期数称为延迟，如果在延迟期间，每个时针周期内，束调度器有一些指令可为某些束发射就可获得完全的利用，或换句话说，每个束的延迟可被其它束完全隐藏。要多少条指令才能隐藏L个时钟的延迟依赖各个指令的吞吐量（参见\ref{sec:arithinst}了解不同指令的吞吐量）。对于所有指令的最大吞吐量。
\begin{itemize}
\item 对于计算能力1.x要L/4（最近取整）条指令，因为多处理器每四个时针周期为每束发射一条该指令，如\ref{sec:cc1x}所述。
\item 对于计算能力2.0要L条指令，因为多处理器每两个时钟周期为每两个束每次每束发射一条指令，如\ref{sec:cc2x}所述。
\item 对于计算能力2.1要2L条指令，因为多处理器每两个时钟周期为两个束每束每次发射两条指令，如\ref{sec:cc2x}所述。
\item 对于计算能力3.x需要8L条指令，因为多处理器每时钟周期为四个束每个束发射两条指令，如\ref{sec:cc3x}所述。
\end{itemize}

对于计算能力2.0的设备，每个周期为两个不同的束发射两条指令，每束一条。对于计算能力2.1的设备，每个周期为两个不同的束发射两对指令，每束发射一对。

对于计算能力3.x的设备，每个周期为四个束发射四对指令，每束一对。

束没有准备好执行它下条指令的最常见原因是指令的输入操作数没有准备好。

如果所有输入操作数是寄存器，延迟是因为寄存器依赖，也就是说，一些输入操作数是由一些还没有完成的在前面的指令写的。在紧接着的寄存器依赖的情况下（一些输入操作数是由前面的指令写的），延迟等于前面指令的执行时间且在此期间束调度器必须为不同的束调度指令。指令不同，执行时间也会变化，但是典型地大约22个时钟周期，这需要计算能力1.x设备上的6个束，计算能力2.x设备上的22个束，计算能力3.x设备上的44个束来隐藏，或者需要更多的束（因为我们假设指令是按照最大吞吐量运算的）。对于计算能力2.1及以上的设备这也要求具有足够的指令级并行，使得调度器能够为每个束发射一对指令。

如果某些输入操作数在片下存储器中，延迟更高：对于计算能力1.x和2.x大约400到800时针周期；对于计算能力3.x大约200到400个周期。在如此高的延迟期保持束调度器繁忙需要的束数依赖于内核代码和指令级并行度；一般，如果没有片下存储器的操作数的指令（即大多数时候指算术指令）数与片下存储器的操作的指令比率小的话（这个比率经常称为程序的运算密度），要求更多束。比如，假设比率是30，在1.x和2.x的设备上片下存储器延迟为600个周期，在3.x的设备上，片下存储器的延迟为300个周期。为了隐藏延迟，对于计算能力1.x的设备大约要5个束，对于计算能力2.x的设备大约20个束，对于计算能力3.x的设备大约40个束。

另一个束没有准备好执行下一条指令的原因是在等待一些存储器栅栏（见\ref{sec:memfence}）或同步点（见\ref{sec:memfence}）。一个同步点能强制多处理器空闲，因为许多束要等待同一块内其它束完成同步点之前的指令。一个多处理器上有多个常驻块能够减少这种情况下的闲置，因为不同块内的束不用在同步点等待。

对于给定的内核调用，一个多处理器上的块和束数目依赖调用时的执行配置（参见\ref{sec:executionconfiguration}），多处理器的存储器资源和如\ref{sec:hardwarethread}描述的内核资源要求。为了帮助程序员基于寄存器和共享存储器要求选择合适的线程块尺寸，CUDA软件开发工具包提供了一个称为CUDA占有率计算器的电子表格，在这里，占有率定义为常驻块数目和最大常驻块数目之比（\ref{sec:cc}为各种计算能力给出了数据）。

寄存器、本地存储器、共享存储器和常量存储器的使用可在编译时使用---ptxas-options=-v选项报告。

一个块内需要的总共享存储器数目等于静态分配的和动态分配的共享存储器之和，另外在计算能力1.x的设备上，还包括传输内核参数需要的共享存储器用量（参见\ref{sec:noinline_inline}）。

内核使用的寄存器数目对常驻束数目有显著影响。例如，计算能力1.2的设备，如果内核使用了16个寄存器且每个块有512个线程且要求非常少的共享存储器，这样多处理器可常驻两个块(即32束)，因为它们要求2*512*16个寄存器，这匹配多处理器的可用寄存器数目。但是如果只要内核多使用一个寄存器，就只有一个块（16束）能够常驻多处理器上了，因为两个块要求2*512*17个寄存器，这大于多处理器的可用寄存器数目。因此编译器试图在保证寄存器溢出的前提下最小化寄存器使用和指令数目。可以使用-maxrregcount编译选项或如\ref{sec:launchbound}描述的发射绑定控制寄存器使用。

每个双精度变量（在支持本地双精度的设备上，即计算能力1.3或更高的设备）和每个long long变量使用两个寄存器。但是计算能力1.2或以上的寄存器总量是低计算能力的至少两倍。

对于一个给定的内核，执行配置对性能的影响依赖于内核代码。推荐实验后确定。应用也可基于寄存器文件和共享存储器尺寸参数化执行配置，这依赖于设备的计算能力，也依赖于多处理器的数目和设备存储器带宽，所有的这些都可以可以使用运行时API和驱动API查询（参见参考手册）。

如果可能每个块的线程数应当是束尺寸的整数倍以避免因为束内线程不足而浪费计算资源。

\section{最大化存储器吞吐量}

最大化应用的总体存储器吞吐量的第一步是最小化低带宽的数据传输。

这意味着要最小化主机存储器和设备存储器间的数据传输，详见\ref{sec:datatransfer}，因为主机和设备间的带宽比全局存储器和设备之间数据传输的带宽要少。

也意味着通过最大化片上存储器使用以最小化全局存储器和设备间的数据传输：如共享存储器和缓存（如计算能力2.x的设备上的L1/L2缓存，所有设备上的纹理缓存和常量缓存）。

共享存储器等价于用户管理的缓存：应用显式的分配和访问它。如\ref{sec:runtime}节所描述的，一个常用的编程模式是将来自设备存储器的数据存储到共享存储器，换句话说，让块内的每个线程：
\begin{itemize}
\item 从设备存储器中加载数据到共享存储器，
\item 同步块内的其它线程以便每个线程能够安全读其它线程写的数据，
\item 在共享存储器内处理数据，
\item 如果需要的话再次同步以保证以结果更新了共享存储器，
\item 将结果写回设备存储器
\end{itemize}

对于一些应用（如对于那些访问全局存储器是数据依赖的），传统的硬件管理的缓存更适用于发掘数据局部性。如\ref{sec:cc2x}所提到的，对于计算能力2.x的设备，同一片上存储器用于L1和共享存储器，至于在每个内核调用时为L1和共享存储器各分配多少则可以配置。

内核的存储器访问吞吐量依赖于每种类型存储器的访问模式，其性能可相差一个数量级。最大化存储器吞吐量的下一步是依据\ref{sec:devicememaccess}描述的最优化存储器访问模式重新组织存储器访问。对于全局存储器来说，这种优化非常重要，因为全局存储器的带宽比较低，所以非优化的全局存储器访问对性能有更大的影响。

\subsection{主机和设备的数据传输}
\label{sec:datatransfer}
应用应当尽力最小化主机和设备间的数据传输。达到这种目标的一种方法是将更多的代码从主机上移到设备上，即使这意味着内核运行低并行度的计算。中间数据结构可以在设备上建立，被设备操作和销毁而用不着被主机映射或复制到主机存储器。

另外由于每次传输的消耗，将多次小的传输组合成一次大的传输会比单独传输更好。

对于有前端总线的系统，使用\ref{sec:pagelocked}描述的分页锁定主机存储器进行主机和设备间的数据传输会获得更好的性能。

另外，在使用被映射主机存储器时（参见\ref{sec:mappedmem}），没有必要分配任何设备存储器和显式在主机和设备间传输数据。数据传输会在每次内核访问映射存储器时隐式进行。为了最大化性能，这些存储器访问必须像全局存储器一样满足合并访问要求（参见\ref{sec:devicememaccess}）。假定被映射主机存储器只被读写一次，使用被映射主机存储器性能会提高。

在集成系统上，设备存储器和主机存储器在物理上是相同的，任何设备和主机间的数据传输都是多余的，此时应当使用被映射主机存储器。应用可以检查integrated设备属性(参见\ref{sec:deviceemu})来查询设备是不是集成的，如果是，返回1。

\subsection{设备存储器访问}
\label{sec:devicememaccess}

访问可寻址空间（即，全局，本地，共享，常量或纹理存储器）的指令可能要被多次重新发射，这依赖于在一个束内线程访问的地址的分布。对于每种存储器这些分布如何影响指令吞吐量是由存储器种类决定的，这在下节描述。例如，对于全局存储器，作为一个通用的准则，地址越分散，吞吐量越小。

\subsubsection{全局存储器}

全局存储器在设备存储器中且设备存储器通过32，64或128个字节存储器通信访问。这些存储器访问是天然对齐的：只有32，64或128字节的设备存储器片段是对齐到它自身的尺寸(也就是首地址是尺寸的整数倍)，它们可以在一次通信中被读写。

当一个束执行一条访问全局存储器的指令时，它会合并束内线程的存储器的访问成一次或多次，这依赖于每个线程访问的字的尺寸和线程间存储器地址分布。一般通信越多，除线程访问的字外传输的不用的字就越多，相应的降低了指令吞吐量。如果每个线程访问4字节产生了32字节的通信，吞吐量要除以8.

要多少次通信和对吞吐量的最终影响都随计算能力变化。对于计算能力1.0和1.1的设备，线程间地址的分布满足合并访问的要求是非常严格的。更高计算能力的设备上就宽松了许多。对于计算能力2.x的设备，存储器通信是缓存的，挖掘了数据局部性以减少合并访问对性能的影响。\ref{sec:cc1x}、\ref{sec:cc2x}和\ref{sec:cc3x}给出了全局存储器访问如何被各种计算能力的设备处理的细节。

为了最大化全局存储器吞吐量，最大化合并访问非常重要：
\begin{itemize}
\item 遵从\ref{sec:cc1x}、\ref{sec:cc2x}和\ref{sec:cc3x}的最优访问模式，
\item 使用满足尺寸和对齐要求的数据类型，详见\ref{sec:alignedRequirement}节
\item 在某些情况下，填充数据，如\ref{sec:access2Dmem}描述的访问二维数组。
\end{itemize}

\textbf{尺寸和对齐要求}
\label{sec:alignedRequirement}

全局存储器指令支持读写长度为1、2、4、8或16字节的字。任何对全局存储器的数据访问（通过变量或指针）编译成一次单独的全局存储器指令当且仅当数据类型的尺寸为1、2、4、8或16字节并且数据是天然对齐的（即地址是尺寸的倍数）。

如果尺寸和对齐要求没有满足，访问被编译成交叉访问的多条指令，这不能完全合并。因此建议对全局存储器中的数据使用满足要求的数据类型。

像float2或float4这样的内置数据类型自动满足对齐要求。

对于结构体，尺寸和对齐要求可用对齐修饰符{\_}{\_}align{\_}{\_}(8)或{\_}{\_}align{\_}{\_}(16)让编译器保证。如

\begin{lstlisting}
struct __align__(8) {

    float x;

    float y;

};
or

struct __align__(16) {

    float x;

    float y;

    float z;

};
\end{lstlisting}

任何全局存储器中的变量的地址或驱动API或运行时API中的存储器分配例程返回的地址总是对齐到至少256字节。

读没有天然对齐的8字节或16字节的字可能会产生错误的结果（偏离一些字），所以要特别注意保证任何值或数组的起始地址对齐。一个典型的容易忽视的例子是使用一些自定义的全局存储器分配模式，如使用一次大的分配并为各数组划分分配的存储器以替代多个数组的分配（使用多次cudaMalloc()或cuMemAlloc()），这种情况下，每个数组的起始地址是偏离块的起始地址的。

\textbf{二维数组}
\label{sec:access2Dmem}

一个常见的全局存储器访问模式是各个索引为(tx,ty)的线程使用下面的地址访问类型为type*、位于地址BaseAddress、宽为width的二维数组的一个元素：

\begin{lstlisting}
BaseAddress + width * ty + tx
\end{lstlisting}

为了全部满足合并访问，width和线程块的宽度必须是束的整数倍（或对于计算能力1.x的设备是半束的整数倍）。

特别地，这意味着宽度不是束的整数倍的数组，在分配空间的时候行向上填充到最近的束的整数倍时，会更有效。手册中的cudaMallocPitch()和cuMem-AllocPitch()函数及相关的存储器复制函数保证程序员写出硬件无关的代码以分配服从这些限制的数组。

\subsubsection{本地存储器}

本地存储器访问仅对某些自动变量发生，如\ref{sec:vartype}所述。编译器可能放入本地存储器的自动变量是：
\begin{itemize}
\item 不能确定是用常数索引的数组，
\item 可能消耗太多寄存器空间的大结构或数组，
\item 如果内核使用了超过可用的寄存器的任何变量（也称为寄存器溢出）。
\end{itemize}

检查PTX汇编代码（编译时使用-ptx或-keep选项获得）可以分辨，如果一个变量在编译的第一阶段被放入本地存储器，会使用.local助记符声明，使用ld.local和st.local访问。即使此时没有放入本地存储器中，在随后的编译阶段，如果觉得对于目标架构它消耗了太多的寄存器，也会被放入本地存储器中：使用cuobjdump检查cubin对象会分辨是不是这种情况。另外编译时使用---ptxas-options=-v选项，编译器会报告内核总的本地存储器用量（lmem）。注意一些数学函数有的实现路径也可能访问了本地存储器。

本地存储器存在于设备存储器空间，所有本地存储器访问延迟像全局存储器一样高，带宽和全局存储器一样低，且服从于\ref{sec:devicememaccess}描述的存储器合并访问要求。本地存储器的组织使得连续的线程ID访问连续的32位字。只要束内所有线程访问同一相对地址（如数组变量的同一索引，结构体变量的同一成员）访问就可完全合并。

在计算能力2.x的设备上，本地存储器访问总是以和全局存储器访问（参见\ref{sec:cc2x}）同样的方式被缓存在L1和L2。

\subsubsection{共享存储器}

由于共享存储器位于芯片上，因而共享存储器空间比本地和全局存储器空间的速度都要快得多。

为了获得较高的存储器带宽，共享存储器被划分为多个大小相等的存储器模块，称为存储体（bank），存储体可同步被访问。因此，对落入n个不同存储体的n个地址的任何存储器读取或写入请求都可同时实现，整体带宽可达到单独一个模块的带宽的n倍。

但若一个存储器请求的两个地址落入同一个存储体内，就会出现存储体冲突，访问必须序列化。硬件会在必要时将存在存储体冲突的存储器请求分割为多个不冲突的请求，此时有效带宽将降低为原带宽除以分离后的存储器请求的数量。如果分离后的存储器请求数量为n，就可以说初始存储器请求导致了n路存储体冲突。

为了获得最大化的性能，理解存储器地址如何映射到存储体以调度存储器请求，以最小化存储体冲突就很重要。这些在\ref{sec:cc1x}、\ref{sec:cc2x}和\ref{sec:cc3x}节为计算能力1.x、计算能力2.x和计算能力3.x分别描述。

\subsubsection{常量存储器}

常量存储器空间存在于设备存储器并被缓存到常理缓存中，参见\ref{sec:cc1x}和\ref{sec:cc2x}。

对于计算能力1.x设备，一个束的常量存储器请求首先被分成两个请求，每半束一个，独立发射。

一个请求然后分成多个请求，使得它们访问的地址不同（译者注：也就是说各个请求只访问一个地址），吞吐量减少到一个等于独立请求的数量的因子。

如果缓存命中的话，最终的请求等于常量缓存的吞吐量，否则等于设备存储器的吞吐量。

\subsubsection{纹理和表面存储器}

纹理和表面存储器空间存在于设备存储器中，并被缓存到纹理缓存，因此纹理获取或表面读仅需在缓存丢失时读取一次设备存储器，否则只需花费一次纹理缓存读取。纹理缓存已为二维空间局部性而优化，因此同一个束的线程读取二维相邻纹理或表面地址的将实现最高性能。此外，它设计用于以固定的延迟执行流获取，一次缓存命中将减少
DRAM 带宽要求，而非延迟。

与从全局存储器或常量存储器读取设备存储器的方法相比，通过纹理或表面获取读取设备存储器可能是一种更有优势的替代方法。
\begin{itemize}
\item 如果存储器读取不符合全局或常量存储器读取模式，纹理存储器会得到好性能，如果在纹理获取的时候存在局部性，能得到更高的带宽；
\item 地址计算在内核外由特定单元处理；
\item 在一个操作中包装的数据可能在一次操作中广播到独立的变量；
\item 8位和16位整形输入数据可能转化为32位浮点值，范围为[0.0，1.0]或[-1.0，1.0]（参见\ref{sec:textureAndSurfacemem}）。
\end{itemize}

\section{最大化指令吞吐量}

为了最大化指令吞吐量，应用应当：
\begin{itemize}
\item 最小化吞吐量较低的指令的使用；包括为速度牺牲精度，如果不影响最终结果的话，如使用内置函数取代常规函数（内置函数列在\ref{sec:intrinsic}），单精度取代双精度，或将非规格化数刷为0。
\item 最小化由流控指令引起的束内分支（参见\ref{sec:controlflow}）；
\item 减少指令数目，例如，如\ref{sec:synchronizefunction}所描述的只要可能就优化同步点或如\ref{sec:restrictpointer}所描述的使用受限的指针。
\end{itemize}

本节，吞吐量以每时钟周期每个多处理器操作数目给定。对于尺寸为32的束，一条指令产生32个操作。因此，如果T是每个时钟周期操作数目，指令吞吐量就是每32/T时钟周期一个指令。

所有吞吐量是对一个多处理器而言。它们可以乘以设备中的多处理器个数以获得总个设备的吞吐量。

\subsection{算术指令}
\label{sec:arithinst}

\ref{tab:instructionthroughput}给出了各种计算能力的设备其硬件本地支持的算术指令吞吐量。

\begin{table}[htbp]
\caption{原生算术指令吞吐量/每时钟每流多处理器操作数目}
\begin{tabular}
{|p{106pt}|p{50pt}<{\centering}|p{50pt}<{\centering}|p{50pt}<{\centering}|p{50pt}<{\centering}|p{50pt}<{\centering}|p{50pt}<{\centering}|}
\hline
\raisebox{-1.50ex}[0cm][0cm]{}&
\multicolumn{6}{|p{315pt}<{\centering}|}{计算能力}  \\
\cline{2-7}  & 1.0/1.1/1.2& 1.3& 2.0& 2.1& 3.0& 3.5 \\
\hline
32-bit floating-point add, multiply, multiplyadd& 8&8 & 32 &48 &192 & 192 \\
\hline
64-bit floating-point add, multiply, multiplyadd& 1& 1& 16(*)& 4& 8&  64\\
\hline
32-bit integer add& 10& 10& 32& 48& 160&   160\\
\hline
32-bit integer compare&10 &10 &32 &48 &160 &160  \\
\hline
32-bit integer shift& 8&8 & 16& 16& 32&  64\\
\hline
Logical operations& 8& 8&32&48&160&160 \\
\hline
32-bit integer multiply, multiplyadd, sum of absolute difference&
 多条指令&多条指令&16&16&32&32 \\
\hline
24-bit integer multiply(\textsf{{\_}{\_}[u]mul24})&
8&8&多条指令&多条指令&多条指令&多条指令 \\
\hline
32-bit floating-point reciprocal, reciprocal square root, base-2 logarithm(\textsf{{\_}{\_}log2f}), base 2 exponential(\textsf{exp2f}), sine(\textsf{{\_}{\_}sinf}), cosine(\textsf{{\_}{\_}cosf})&2&2&4&8&32&32 \\
\hline
Type conversions from 8-bit and 16-bit integer to 32-bit types&8&8&16&16&128&128\\
\hline
Type conversions from and to 64-bit types&多条指令&1&16(*)&4&8&32\\
\hline
All other type conversions&8&8&16&16&32&32\\
\hline
\end{tabular}
\label{tab:instructionthroughput}
\end{table}

其它指令和函数基于本地指令实现。对于计算能力1.x和2.x的设备其实现可能不同，而不同的编译器版本编译后的本地指令数量也可能波动。对于复杂函数，可能有依赖于输入的多条代码路径。可以使用cuobjdump检查cubin对象中的一个特定实现。

一些函数的实现在CUDA头文件中(math{\_}functions.h，device{\_}functions.h等)可以找到。

一般，代码使用-ftz=true（非正规化数刷到）编译比使用-ftz=false编译的性能要高。同样的代码使用-prec-div=false（低精度除法）编译的性能比-prec-div=true编译的性能高，和代码使用-prec-sqrt=false（低精度平方根）编译比使用-prec-sqrt=true编译性能要高。nvcc用户手册更详细说明了这些标签。

\textbf{单精度浮点加和内置乘}

{\_}{\_}fadd{\_}r[d,u], {\_}{\_}fmul{\_}r[d,u], and{\_}{\_}fmaf{\_}r[n,z,d,u]（参见\ref{sec:arithinst})，对于计算能力1.x的设备，编译成几十个指令，但对于计算能力2.x及以上的设备映射成单条本地指令

\textbf{单精度浮点除}

{\_}{\_}fdividef(x, y) (参见\ref{sec:arithinst})提供了比除法操作符快的单精度浮点除法


\textbf{单精度浮点倒数平方根}

为了保存IEEE-754的语义，只有当倒数和平方根都是近似的时候（即-prec-div=false和-prec-sqrt=false），编译器才将1.0/sqrtf()优化为 rsqrtf()。所以应当在需要的时候直接调用rsqrtf()。

\textbf{单精度浮点平方根}

单精度浮点平方根实现为倒数平方根的倒数而不是倒数平方根的乘法，因此对和无穷能得到正确的结果。

\textbf{正弦和余弦}

sinf(x), cosf(x), tanf(x), sincosf(x)和对应双精度指令比较昂贵，如果x的值大的话会更为昂贵。

更精确地，参数归约代码包含两个称为快速路径和慢路径的代码。

快速路径用于参数足够小且本质上只包含一些乘加操作。慢路径用于参数大且包含长计算要求以获得整个参数范围内的正确结果。

目前，三角任何函数的参数归约代码在参数小于48039.0f时为单精度函数选择快路径，小于2147483648.0时为双精度选择快路径。

因为慢路径相比快路径要更多的寄存器，故将中间结果放到本地存储器中试图减少寄存器的用量，由于本地存储器的高延迟和低带宽（参见\ref{sec:devicememaccess}），可能影响性能。目前，为单精度使用了28字节的本地存储器，而双精度使用了44字节。但具体的数量可能会改变。

由于慢路径计算量长和使用了本地存储器，任何三角函数的吞吐量慢路径比快路径相差一个数量级。


\textbf{整数算术}

在计算能力1.x的设备上，32位整数乘法是使用非本地支持的乘法指令实现的。24位整形乘法是通过{\_}{\_}[u]mul24内置指令（参见C.2.3节）本地支持的。在指令限制的内核中，只要可能就使用{\_}{\_}[u]mul24取代32位乘法操作符，性能一般会得到提升的。但是如果使用{\_}{\_}[u]mul24阻止了编译器优化，也可能得到相反的效果。

在计算能力2.x及以上的设备上，32位整数乘法是本地支持的，而24位不是。{\_}{\_}[u]mul24使用多条指令使用，因此不应当使用。

整数除法和模余非常昂贵：计算能力1.x的设备上要几十条指令，而计算能力2.x及以上的设备上少于二十条指令。只要可能就应当避免使用或用位操作符取代：如果n是2的乘方，（i/n）等于(i>>log2(n))，而(i{\%}n)等于(i{\&} (n-1))；如果n是字面量，编译器会自动实现转换。

{\_}{\_}brev、{\_}{\_}brev11、{\_}{\_}popc和{\_}{\_}popcll，对于计算能力1.x编译成几十个指令，但是对于计算能力2.x{\_}{\_}brev和{\_}{\_}popc映射成一条指令，而{\_}{\_}brell和{\_}{\_}popcll就几条指令。

{\_}{\_}clz、{\_}{\_}clzll、{\_}{\_}ffs和{\_}{\_}ffsll编译成的指令数在计算能力2.x及以上的设备上比计算能力1.x的要少。

\textbf{类型转换}

有时，编译器会插入类型转换指令，这增加了执行周期。下面的就是这种情况：
\begin{itemize}
\item 函数操作的变量的类型为char或short，这的操作数一般会被转化为int。
\item 双精度浮点常量（即没有类型后缀的常量定义）用作单精度计算的输入。
\end{itemize}

后面一种情况可以使用单精度浮点常量避免，使用后缀f定义，如3.14159-2653589793f, 1.0f, 0.5f。

\subsection{控制流指令}
\label{sec:controlflow}

任何流控制指令（if、switch、do、for、while）都会导致同一束的线程分支（即走向不同的执行路径），从而显著影响有效指令吞吐量。如果出现这种情况，不同的执行路径必须序列化，因而增加了为该束执行的指令总数。当完成所有不同的执行路径时，线程将重新汇聚到同一执行路径。

为了在控制流依赖线程 ID的情况下获得最佳性能，应控制条件以最小化分支束的数量。这是可行的，因为束在块内的分布情况是确定的，如\ref{sec:simt}所提到的。一个简单的例子就是，当控制条件仅依赖于 (threadIdx / warpSize)时，其中的 warpSize是束的大小，这种情况下，不会出现任何束内分支，因为控制条件与束完美对齐。

有些时候，编译器可能会展开循环或使用分支谓词来优化 if 或 switch语句，下面将详细说明。在这些情况下，不会有任何 warp分支。程序员还可使用
{\#}pragma unroll 伪指令控制循环的展开（请参见\ref{sec:unroll}）。

在使用分支谓词时，依靠控制条件执行的任何指令都不会被跳过。而是分别与一个每线程条件代码或根据控制条件设置为true 或 false 的谓词相关联，尽管每一条指令都为执行而进行了调度，但只有谓词为true 的指令才会被实际执行。带有 false谓词的指令不会写入结果，也不会计算地址或读取操作数。

只有在分支条件控制的指令数量小于或等于特定阈值时，编译器才会使用有谓词的指令替换分支指令：如果编译器确定出有可能产生大量分支束的条件，则此阈值为
7，否则为 4。

\subsection{同步指令}

对于计算能力1.x的设备，{\_}{\_}syncthreads()是每个时钟周期8个操作，对于计算能力2.x的设备每时钟周期16个操作，对于计算能力3.x的设备，每周期128个操作。

注意；{\_}{\_}syncthreads()能通过强制某些多处理器闲着从而影响性能，细节见\ref{sec:devicememaccess}。

